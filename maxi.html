<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Unsupervised finetuning of VL models for zero-shot and few-shot action recognition">
  <meta name="keywords" content="MAXI">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MAXI</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!--<link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://wlin-at.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://jmiemirza.github.io/ActMAD/">
            ActMAD
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MAtch, eXpand and Improve: Unsupervised Finetuning for Zero-Shot Action Recognition with Language Knowledge</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://wlin-at.github.io/">Wei Lin</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=WbO7tjYAAAAJ&hl=en">Leonid Karlinsky</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=qZtU1L4AAAAJ&hl=en">Nina Shvetsova</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://snototter.github.io/research/">Horst Possegger</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=oDDqnQ4AAAAJ&hl=en">Mateusz Kozinski</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://rpand002.github.io/">Rameswar Panda</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.rogerioferis.org/">Rogerio Feris</a><sup>2</sup>
            </span>
            <span class="author-block">
              <a href="https://hildekuehne.github.io/">Hilde Kuehne</a><sup>2,3</sup>
            </span>
            <span class="author-block">
              <a href="https://www.tugraz.at/institute/icg/research/team-bischof/people/team-about/horst-bischof">Horst Bischof</a><sup>1</sup>
            </span>            
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Institute of Computer Graphics and Vision, Graz University of Technology, Austria</span>
            <span class="author-block"><sup>2</sup>MIT-IBM Watson AI Lab, USA</span>
            <span class="author-block"><sup>3</sup>Goethe University Frankfurt, Germany</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2303.08914.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2303.08914"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="dummy"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/wlin-at/MAXI"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!--
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
                  -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>







<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large scale Vision-Language (VL) models have shown tremendous success in aligning representations between visual and text modalities. This enables remarkable progress in zero-shot recognition, image generation & editing, and many other exciting tasks. However, VL models tend to over-represent objects while paying much less attention to verbs, and require additional tuning on video data for best zero-shot action recognition performance. 
          </p>
          <p>
            While previous work relied on large-scale, fully-annotated data, in this work we propose an unsupervised approach. We adapt a VL model for zero-shot and few-shot action recognition using a collection of unlabeled videos and an unpaired action dictionary. Based on that, we leverage Large Language Models and VL models to build a text bag for each unlabeled video via matching, text expansion and captioning. We use those bags in a Multiple Instance Learning setup to adapt an image-text backbone to video data. 
          </p>
          <p>
            Although finetuned on unlabeled video data, our resulting models demonstrate high transferability to numerous unseen zero-shot downstream tasks, improving the base VL model performance by up to 14%, and even comparing favorably to fully-supervised baselines in both zero-shot and few-shot video recognition transfer.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    
    
<section class="section">
  <div class="container is-max-desktop">
    <!-- Attention Heatmaps -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <hr>
        <h2 class="title is-3">Attention Heatmaps</h2>
          
         <img src="media/maxi/attention_in_dict.png" class="center"/>
        <div class="content has-text-justified">
          <br>  
          <p>
            Attention heatmaps on actions which have a verb form (lemma or gerund) directly included in the predefined action dictionary. We compare among CLIP (2nd row), ViFi-CLIP (3rd row) and our MAXI (4th row). Warm and cold colors indicate high and low attention. MAXI has more focused attention on hands (for clap) and legs (for kick ball).
          </p>
          </div>
          
          <img src="media/maxi/attention_novel.png" class="center"/>
        <div class="content has-text-justified">
          <br>  
          <p>
            Attention heatmaps on actions which do have any verb form included in the predefined action dictionary. MAXI has more focused attention on hand and arm for wave, and on the area of mouth for chew.
          </p>
        </div>
        <br>
         <img src="media/maxi/attention_in_general.png" class="center"/>
        <div class="content has-text-justified">
          <br>  
          <p>
            Attention heatmaps on general actions whose verb form is a basic component of several actions in the predefined action dictionary. MAXI has more concentrated attention on the part where the action happens, e.g. catching ball with hands, hitting drum with stick, legs and feet jump on stairs, and attention on the running body. 
          </p>
          
          
        </div>
      </div>
    </div>
    <hr>
    
    
    
    <!-- Examples of Text Bag -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <hr>
        <h2 class="title is-3">Examples of Text Bag</h2>
          
        <img src="media/maxi/examples_language_sources.png" class="center"/>
        <div class="content has-text-justified">
          <br>  
          <p>
            Examples of video frames, BLIP frame captions, GPT-3 phrases, together with the derived BLIP verb bag and GPT-3 verb bag. The videos are from the K400 dataset.
          </p>
        </div>
          
      </div>
    </div>
    <hr>
    

 

</div>
</section>    
 



    
    
    <!-- Paper video. -->
    <!--
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    -->
    <!--/ Paper video. -->
  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{lin2023match,
  title={MAtch, eXpand and Improve: Unsupervised Finetuning for Zero-Shot Action Recognition with Language Knowledge},
  author={Lin, Wei and Karlinsky, Leonid and Shvetsova, Nina and Possegger, Horst and Kozinski, Mateusz and Panda, Rameswar and Feris, Rogerio and Kuehne, Hilde and Bischof, Horst},
  journal={arXiv preprint arXiv:2303.08914},
  year={2023}
}
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/wlin-at" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>. The template is from <a rel="license"
                                                href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
